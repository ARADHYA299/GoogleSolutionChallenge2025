{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ARADHYA299/GoogleSolutionChallenge2025/blob/main/GenAIfinancialassistant.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hH6rBCYTjgPl",
        "outputId": "bd05aac5-73dc-4a0c-e557-44be4278d5df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: google-genai\n",
            "Version: 1.4.0\n",
            "Summary: GenAI Python SDK\n",
            "Home-page: https://github.com/googleapis/python-genai\n",
            "Author: \n",
            "Author-email: Google LLC <googleapis-packages@google.com>\n",
            "License: Apache-2.0\n",
            "Location: /usr/local/lib/python3.11/dist-packages\n",
            "Requires: google-auth, httpx, pydantic, requests, typing-extensions, websockets\n",
            "Required-by: \n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.5.1+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.5.1+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install -q google-genai\n",
        "!pip show google-genai\n",
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "KTJkfTPMjssM",
        "outputId": "f7b61094-e428-41c9-ae3f-7d0239db9c44"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: API key 'tutorhfkey' not found in environment variables\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-39a60cc45718>:142: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(\n",
            "<ipython-input-3-39a60cc45718>:142: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://dc1b115386079d1f89.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://dc1b115386079d1f89.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "import gradio as gr\n",
        "import requests\n",
        "import time\n",
        "import os\n",
        "\n",
        "# Get API key from Hugging Face Spaces secret\n",
        "HF_API_KEY = os.environ.get(\"tutorhfkey\")\n",
        "if not HF_API_KEY:\n",
        "    print(\"Warning: API key 'tutorhfkey' not found in environment variables\")\n",
        "    # Fallback only for development\n",
        "    HF_API_KEY = \"YOUR_API_KEY_PLACEHOLDER\"\n",
        "\n",
        "HF_API_URL = \"https://api-inference.huggingface.co/models/mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "headers = {\"Authorization\": f\"Bearer {HF_API_KEY}\"}\n",
        "\n",
        "# System instructions for the financial tutor\n",
        "SYSTEM_INSTRUCTIONS = \"\"\"You are a financial markets tutor and adviser designed to educate and recommend/advise beginners, intermediates or experts about investing, financial instruments, money markets and market dynamics.\n",
        "Answer briefly and to the point, don't explain unnecessarily, and use things like real life examples or the things with which they can relate them.\n",
        "\n",
        "Capabilities:\n",
        "- Teach financial concepts in an interactive and engaging way.\n",
        "- Guide users on different types of financial markets (stocks, bonds, crypto, commodities, etc.).\n",
        "- Explain investment strategies, risk management, and portfolio diversification but tell these only when asked.\n",
        "- Answer questions related to fundamental and technical analysis.\n",
        "- If they give you certain budgets provide them the best areas to invest along with risk involved but warn them about the risks and also advise to think before investing.\n",
        "\n",
        "Guidelines:\n",
        "- Begin by understanding the user's financial knowledge level.\n",
        "- If the user is new, first ask them which financial market they are interested in.\n",
        "- Provide structured, step-by-step explanations with examples.\n",
        "- Use simple language and try to give response in an example or things with which the user can relate.\"\"\"\n",
        "\n",
        "# Function to query the model with retry logic\n",
        "def query_model(prompt, max_retries=3):\n",
        "    \"\"\"Send a request to the Hugging Face API with retry logic\"\"\"\n",
        "    payload = {\n",
        "        \"inputs\": prompt,\n",
        "        \"parameters\": {\n",
        "            \"max_new_tokens\": 512,\n",
        "            \"temperature\": 0.7,\n",
        "            \"top_p\": 0.95\n",
        "        }\n",
        "    }\n",
        "\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = requests.post(\n",
        "                HF_API_URL,\n",
        "                headers=headers,\n",
        "                json=payload,\n",
        "                timeout=30\n",
        "            )\n",
        "\n",
        "            # Handle successful response\n",
        "            if response.status_code == 200:\n",
        "                return response.json()[0][\"generated_text\"]\n",
        "\n",
        "            # Handle model loading\n",
        "            elif response.status_code == 503:\n",
        "                wait_time = 2 * (attempt + 1)\n",
        "                print(f\"Model is loading. Waiting {wait_time} seconds...\")\n",
        "                time.sleep(wait_time)\n",
        "\n",
        "                # If last attempt, return a friendly message\n",
        "                if attempt == max_retries - 1:\n",
        "                    return \"The model is currently loading. Please try again in a moment.\"\n",
        "\n",
        "            # Handle other errors\n",
        "            else:\n",
        "                print(f\"Error: Received status code {response.status_code}\")\n",
        "                error_msg = f\"Error: Received status code {response.status_code}. \"\n",
        "                if attempt == max_retries - 1:\n",
        "                    return f\"{error_msg} Please try again later.\"\n",
        "                time.sleep(2)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Exception occurred: {str(e)}\")\n",
        "            if attempt == max_retries - 1:\n",
        "                return f\"Sorry, I encountered an error: {str(e)}. Please try again later.\"\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"I'm having trouble connecting to the server. Please try again later.\"\n",
        "\n",
        "# Format the conversation for the model\n",
        "def format_prompt(messages):\n",
        "    \"\"\"Format the conversation history for the Mistral model\"\"\"\n",
        "    prompt = f\"<s>[INST] {SYSTEM_INSTRUCTIONS}\\n\\n\"\n",
        "\n",
        "    for i, message in enumerate(messages):\n",
        "        role = message[\"role\"]\n",
        "        content = message[\"content\"]\n",
        "\n",
        "        if role == \"user\":\n",
        "            if i == len(messages) - 1:\n",
        "                # Last user message\n",
        "                prompt += f\"{content} [/INST]\"\n",
        "            else:\n",
        "                # User message followed by assistant message\n",
        "                assistant_msg = messages[i+1][\"content\"] if i+1 < len(messages) and messages[i+1][\"role\"] == \"assistant\" else \"\"\n",
        "                prompt += f\"{content} [/INST] {assistant_msg} </s>\"\n",
        "                if i+2 < len(messages) and messages[i+2][\"role\"] == \"user\":\n",
        "                    prompt += \"<s>[INST] \"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Chatbot logic\n",
        "def respond(message, chat_history, conversation_state):\n",
        "    \"\"\"Process user message and get response from the model\"\"\"\n",
        "    # Initialize conversation if empty\n",
        "    if not conversation_state:\n",
        "        conversation_state = [\n",
        "            {\"role\": \"assistant\", \"content\": \"Hello! I'm your financial markets tutor. Before we begin, could you tell me your current level of knowledge about financial markets? Are you a beginner, intermediate, or advanced?\"}\n",
        "        ]\n",
        "\n",
        "    # Add user message to conversation\n",
        "    conversation_state.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    # Format the prompt and get response\n",
        "    prompt = format_prompt(conversation_state)\n",
        "    response_text = query_model(prompt)\n",
        "\n",
        "    # Extract just the model's response part\n",
        "    if \"[/INST]\" in response_text:\n",
        "        response_text = response_text.split(\"[/INST]\")[-1].strip()\n",
        "\n",
        "    # Clean up response text (remove trailing conversation markers)\n",
        "    response_text = response_text.replace(\"</s>\", \"\").strip()\n",
        "\n",
        "    # Add assistant response to conversation\n",
        "    conversation_state.append({\"role\": \"assistant\", \"content\": response_text})\n",
        "\n",
        "    # Update the chat history for Gradio display\n",
        "    chat_history.append((message, response_text))\n",
        "\n",
        "    return chat_history, conversation_state\n",
        "\n",
        "# Create the Gradio interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"# ğŸ“Š Financial Markets Tutor & Adviser ğŸ“Š\")\n",
        "    gr.Markdown(\"Ask questions about financial markets, investing strategies, and more!\")\n",
        "\n",
        "    chatbot = gr.Chatbot(\n",
        "        value=[],\n",
        "        show_label=False,\n",
        "        height=500,\n",
        "        bubble_full_width=False,\n",
        "        show_copy_button=True,\n",
        "    )\n",
        "\n",
        "    conversation_state = gr.State([])\n",
        "\n",
        "    with gr.Row():\n",
        "        message = gr.Textbox(\n",
        "            show_label=False,\n",
        "            placeholder=\"Type your question here...\",\n",
        "            scale=5\n",
        "        )\n",
        "        submit = gr.Button(\"Send\", scale=1)\n",
        "\n",
        "    with gr.Accordion(\"About this Tutor\", open=False):\n",
        "        gr.Markdown(\"\"\"\n",
        "        ### Tips for using the Financial Tutor:\n",
        "        - Start by sharing your experience level (beginner, intermediate, advanced)\n",
        "        - Ask specific questions about markets, instruments, or strategies\n",
        "        - For investment advice, mention your budget and risk tolerance\n",
        "        - Be patient - the model may take a moment to respond on first use\n",
        "        \"\"\")\n",
        "\n",
        "    # Initialize the chat with a greeting\n",
        "    def initialize_chat():\n",
        "        initial_message = \"Hello! I'm your financial markets tutor. Before we begin, could you tell me your current level of knowledge about financial markets? Are you a beginner, intermediate, or advanced?\"\n",
        "        return [(None, initial_message)], [{\"role\": \"assistant\", \"content\": initial_message}]\n",
        "\n",
        "    # Functions for handling user input\n",
        "    def user_input(message, chat_history, conversation_state):\n",
        "        if message == \"\":\n",
        "            return chat_history, conversation_state, gr.update()\n",
        "        return respond(message, chat_history, conversation_state) + (gr.update(value=\"\"),)\n",
        "\n",
        "    # Wire up the interface components\n",
        "    message.submit(user_input, [message, chatbot, conversation_state], [chatbot, conversation_state, message])\n",
        "    submit.click(user_input, [message, chatbot, conversation_state], [chatbot, conversation_state, message])\n",
        "\n",
        "    # Initialize the chat on page load\n",
        "    demo.load(initialize_chat, [], [chatbot, conversation_state])\n",
        "\n",
        "# Launch the app for HF Spaces\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "22TGOYh99qIf"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsyHJ3v4X2yhOEzLcS/utN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}